\documentclass[12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{fullpage}
\usepackage{amsmath,amsthm,amsfonts,amssymb,bbm}
\usepackage[capitalize,nameinlink]{cleveref}
\usepackage{mathtools}
\usepackage{xcolor}
\usepackage{physics}
\usepackage[shortlabels]{enumitem}

\newcommand{\eps}{\varepsilon}
\newcommand{\bitset}{\{0,1\}}
\newcommand{\fbitset}{\{1,-1\}}
\newcommand{\tritset}{\{0,1,\bot\}}
\newcommand{\E}[2][]{\mathbb E_{#1}\left[#2\right]}
\renewcommand{\P}{\mathbb P}
\newcommand{\F}{\mathbb F}
\newcommand{\N}{\mathbb N}
\newcommand{\Z}{\mathbb Z}
\newcommand{\R}{\mathbb R}
\newcommand{\chf}{\mathbbm{1}}
\newcommand{\poly}{\mathrm{poly}}
\newcommand{\polylog}{\mathrm{polylog}}
\newcommand{\supp}{\mathrm{supp}}
\newcommand{\Inf}{\mathbf{Inf}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\Var}[2]{\mathbf{Var}_{#1}\left[#2\right]}
\newcommand{\Stab}{\mathbf{Stab}}
\newcommand{\set}[1]{\{#1\}}
\newcommand{\ith}{$i^\text{th}$}
\newcommand{\eqn}[1]{\begin{equation*}#1\end{equation*}}
\newcommand{\hf}{\widehat{f}}
\newcommand{\sumi}{\sum_{i \in [n]}}
\newcommand{\sumj}{\sum_{j \in [n]}}
\newcommand{\sumk}{\sum_{k \in [n]}}
\newcommand{\suml}{\sum_{\ell \in [n]}}
\newcommand{\sumx}{\sum_{x \in \bitset^n}}
\newcommand{\sumxf}{\sum_{x \in \fbitset^n}}
\newcommand{\sumS}{\sum_{S \subseteq [n]}}
\renewcommand{\ip}[1]{\left\langle#1\right\rangle}
\DeclareMathOperator{\T}{T}
\newcommand{\TODO}{{\color{red}TODO}}

\begin{document}
\section{Chapter 1}

\subsection{}

\begin{enumerate}[(a)]
    \item \begin{align*}
		     \widehat{\min}_2(00) = \ip{\chi_{00}, f} &= \frac{1}{4}(f(1,1) + f(1,-1) + f(-1,1) + f(-1,-1))\\
		     &= -\frac{1}{2}
	      \end{align*}

	      \begin{align*}
		     \widehat{\min}_2(01) = \ip{\chi_{01}, f} &= \frac{1}{4}(f(1,1) + f(1,-1) - f(-1,1) - f(-1,-1))\\
		     &= \frac{1}{2}
	      \end{align*}

	      \begin{align*}
		     \widehat{\min}_2(10) = \ip{\chi_{10}, f} &= \frac{1}{4}(f(1,1) - f(1,-1) + f(-1,1) - f(-1,-1))\\
		     &= \frac{1}{2}
	      \end{align*}

	      \begin{align*}
		     \widehat{\min}_2(11) = \ip{\chi_{11}, f} &= \frac{1}{4}(f(1,1) - f(1,-1) - f(-1,1) + f(-1,-1))\\
		     &= \frac{1}{2}
	      \end{align*}

	      \begin{align*}
	      	{\min}_2(x,y) &= \frac{1}{2}(-\chi_{00} + \chi_{01} + \chi_{10} + \chi_{11})(x,y)\\
	      	&= \frac{1}{2}(-1 + y + x + xy)
	      \end{align*}

    \item \begin{align*}
		     \widehat{\min}_3(000) = \ip{\chi_{000}, f} &= \frac{1}{8}(f(1,1,1) + f(1,1,-1) + f(1,-1,1) + f(1,-1,-1)\\
		     &\qquad + f(-1,1,1) + f(-1,1,-1) + f(-1,-1,1) + f(-1,-1,-1))\\
		     &= -\frac{3}{4}
	      \end{align*}
	      \eqn{\widehat{\min}_3(001) = \widehat{\min}_3(010) = \widehat{\min}_3(100) = \frac{1}{4}}
	      \eqn{\widehat{\min}_3(011) = \widehat{\min}_3(101) = \widehat{\min}_3(110) = \frac{1}{4}}
	      \eqn{\widehat{\min}_3(111) = \frac{1}{4}}
	      \begin{align*}
	      	{\min}_3(x,y,z) &= \frac{1}{4}(-3\chi_{000} + \chi_{001} + \chi_{010} + \chi_{100} + \chi_{011} + \chi_{101} + \chi_{110} + \chi_{111})(x,y,z)\\
	      	&= \frac{1}{4}(-3 + z + y + x + yz + xz + xy + xyz)
	      \end{align*}
	      \eqn{{\max}_3(x,y,z) = - {\min}_3(-x,-y,-z) = \frac{1}{4}(3 + z + y + x - yz - xz - xy + xyz)}

    \item $1_{\set{a}}(x) = \frac{1}{2^n}\sumS \chi_S(a) \chi_S(x) = \frac{1}{2^n}\sumS \chi_S(a+x)$

    \item $\varphi_{\set{a}}(x) = 2^n 1_{\set{a}}(x) = \sumS \chi_S(a+x)$

    \item $\varphi_{\set{a + e_i}} = 2^{n-1} (1_{\set{a}} + 1_{\set{e_i}}) = \frac{1}{2}\sumS (\chi_S(a) + \chi_S(e_i)) \chi_S(x)$

    \item As the density corresponding to coordinate $i$ is $2^{n-1}\left((1 + \rho) 1_{\set{e_i}} + (1 - \rho)1_{\set{-e_i}}\right)$, we have
	      \begin{align*}
	      	\widehat{\phi}(S) &= \frac{1}{2^n}\prod_{i=1}^n((1+\rho)\chi_S(e_i) + (1-\rho)\chi_S(-e_i))\\
	      	&= \frac{1}{2^n}\prod_{i=1}^n((1+\rho)s_{e_i} - (1-\rho) s_{-e_i})
	      \end{align*}

	      \item \begin{align*}
	      		  \ip{\text{IP}_{2n}, \chi_{a,b}} &= \frac{1}{2^{2n}}\sum_{x, y \in \bitset^n} (-1)^{x \cdot y} (-1)^{a \cdot x + b \cdot y}\\
	      		  &= \frac{1}{2^{2n}}\sumx (-1)^{a \cdot x} \sum_{y \in \bitset^n} (-1)^{y \cdot (x + b)}.
	      		\end{align*}
	     As the inner sum is 0 if $x \neq b$ and $2^n$ otherwise,
	     \eqn{\text{IP}_{2n}(x,y) = \frac{1}{2^n}\sum_{a, b \in \bitset^n} (-1)^{a \cdot b} \chi_{a,b}(x,y)}

    \item As $\ip{\text{EQ}_n,\chi_S} = 2^{-n}(\chi_S(0^n) + \chi_S(1^n))$,
	      \eqn{\text{EQ}_n(x) = 2^{n-1}\sum_{\substack{S \subseteq [n]\\\abs{S} \text{ even}}} \chi_S(x)}
	      (We take the domain to be $\F_2^n$.)

    \item We have $\text{NEQ}_n(x) = 1 + \text{EQ}_n$, so
	      \eqn{\text{NEQ}_n(x) = 2^{n-1}\sum_{\substack{S \subseteq [n]\\\abs{S} \text{ odd}}} \chi_S(x)}

    \item As $\widehat{\text{Sel}}(001) = \widehat{\text{Sel}}(010) = \widehat{\text{Sel}}(101) = - \widehat{\text{Sel}}(110) = \frac{1}{2}$,
		    \eqn{\text{Sel}(x) = \frac{1}{2}(x_2 + x_3 + x_1 x_3 - x_1 x_2)}

    \item Since $\ip{\text{mod}_3, \chi_{S}} = \frac1{8}(\chi_{S}(000) + \chi_{S}(111))$,
	      \eqn{\text{mod}_3 = \frac1{4} \left(1 + \chi_{\set{1,2}} + \chi_{\set{1,3}} + \chi_{\set{2,3}}\right)}

    \item

    \item

    \item

    \item
\end{enumerate}
	
\subsection{}
 $2n+2$, corresponding to the constant and dictator/antidictator functions.
	
\subsection{}
For every $S \subseteq [n]$, we have
\eqn{\ip{f, \chi_S} = \frac1{2^n} \sumx f(x) \chi_S(x) = \sum_{f(x) = 1} \chi_S(x),}
and since $\chi_S(x) \in \fbitset$ and $\abs{\set{x \in \bitset^n : f(x) = 1}}$ is odd, the sum is nonzero.

\subsection{}

\subsection{}

\subsection{}
%If $f = \sumS \hf_1(S) \chi_S = \sumS \hf_2(S) \chi_S$, then Parseval implies $\sumS \hf_1(S) \hf_2(S) = 1$. Since $\abs{\hf_1(S) \hf_2(S)} \leq 1$, equality

\subsection{}
For all $S \subseteq [n]$, we have $\hat{\mathbf{f}}(S) = \ip{\chi_S,\mathbf{f}} = \frac1{2^n}\sumxf\mathbf{f}(x) \chi_S(x)$, so 
\eqn{\E[\mathbf{f}]{\hat{\mathbf{f}}(S)} = \E[\mathbf{f}]{\frac1{2^n}\sumxf \mathbf{f}(x) \chi_S(x)} = \frac1{2^n}\sumxf\chi_S(x) \E{\mathbf{f}}{\mathbf{f}(x)} = 0.}
Moreover,
\begin{align*}
    \Var{}{\mathbf{\hf}(S)} = \E[\mathbf{f}]{\mathbf{\hat{f}}(S)^2} &= \frac1{2^{2n}}\E[\mathbf{f}]{2^n + \sumx \sum_{\substack{y \in \fbitset^n\\ y \neq x}} \chi_S(x) \chi_S(y) \mathbf{f}(x) \mathbf{f}(y)}\\
    &= \frac1{2^n} + \sumx \sum_{\substack{y \in \fbitset^n\\ y \neq x}} \chi_S(x) \chi_S(y) \E[\mathbf{f}]{\mathbf{f}(x) \mathbf{f}(y)}\\
    &= \frac1{2^n}.
\end{align*}

\subsection{}

\subsection{}

\subsection{}

\subsection{}

\subsection{}

\subsection{}

\subsection{}

\subsection{}

\subsection{}

\subsection{}

\subsection{}

\subsection{}

\subsection{}

\subsection{}

\subsection{}

\subsection{}

\subsection{}

\subsection{}

\subsection{}

\subsection{}

\subsection{}

\subsection{}
\begin{enumerate}[(a)]
    \item ($\Longrightarrow$) $f(x) + f(y) + f(z) = a \cdot (x + y + z) + 3b = a \cdot (x + y + z) + b = f(x+y+z)$.
    
    ($\Longleftarrow$) Let $b \coloneqq f(0)$ and $a \coloneqq (f(e_1) - b, f(e_2) - b, \ldots, f(e_n) - b)$. Then,
    \begin{align*}
    f(x) = f\left(\sum_{i = 1}^n x_i \cdot e_i\right) &= f(x_i e_i) + f(0) + f\left(\sum_{i = 2}^n x_i e_i\right)\\
    &= f(x_n e_n) + \sum_{i = 1}^{n-1} f(x_i e_i) + f(0)\\
    &= f(0) + \sum_{i = 1}^{n} f(x_i e_i) + f(0)\\
    &= a \cdot x + b,
    \end{align*}
    since $f(x_i e_i) = \left\{\begin{array}{lll}f(0) &= b, &\text{if } x_i = 0,\\f(e_i) + f(0) &= a_i, &\text{if } x_i = 1\end{array}\right. = a_i x_i$.
    
    \item We have
    \begin{align*}
        \E[x,y,z]{f(x)f(y)f(z)f(x+y+z)} &= \E[x,y]{f(x)f(y)\E[z]{f(z)f(x+y+z)}}\\
        &= \E[x,y]{f(x) f(y) \cdot (f * f)(x+y)}\\
        &= \E[x]{f(x) \E[y]{f(y) (f * f)(x+y)}}\\
        &= \E[x]{f(x) \cdot (f * f * f)(x)}\\
        &= \ip{f, f * f * f}\\
        &= \ip{\hat{f}, \widehat{f * f * f}}\\
        &= \sumS \hat{f}(S)^4.
    \end{align*}
    
    \item Consider the test that samples $x, y, z \sim \bitset^n$ uniformly and independently, queries $f(x)$, $f(y)$, $f(z)$ and $f(x + y + z)$, accepting if and only if $f(x+y+z) = f(x) + f(y) + f(z)$. Encoding the output of $f$ by $\fbitset$, the expression$\frac1{2} + \frac1{2} f(x) f(y) f(z) f(x+y+z)$ is the indicator of the test's acceptance. Then,
    \eqn{\P[\text{test accepts}] = \frac1{2} + \frac1{2} \sumS \hat{f}(S)^4 \geq 1 - \eps}
    implies $1 - 2\text{dist}(f, \chi_T) = \hat{f}(T) \geq \hat{f}(T)^2  = \hat{f}(T)^2 \cdot \sumS \hat{f}(S)^2 \geq \sumS \hat{f}(S)^4 \geq 1 - 2\eps$ for some $T \subseteq [n]$. Thus, $\text{dist}(f, \chi_T) \leq \eps$.
    
    \item It is clear that if $f(x) + f(y) + f(z) = f(x + y + z)$ holds for all $x,y,z$, then it does in particular when $z = 0$. Conversely, if we are only ensured of $f(x) + f(y) + f(0) = f(x + y)$, then $f(x + y + z) = f(x + y) + f(z) + f(0) = f(x) + f(y) + f(z) + 2f(0) = f(x) + f(y) + f(z)$; therefore, by (a), $f$ is affine if and only if $f(x+y) = f(x) + f(y) + f(0)$ for all $x,y$.
    
    The test samples $x,y \sim \bitset^n$ uniformly and independently, queries $f(x)$, $f(y)$, $f(0)$ and $f(x+y)$, accepting when $f(x+y) = f(x) + f(y) + f(0)$. Shifting to $\fbitset$ notation for the codomain of $f$, the indicator of acceptance is $\frac1{2} + \frac{f(0)}{2} \cdot f(x) f(y) f(x+y)$ and the same analysis of the BLR test works (the factor $f(0)$ disappears when taking absolute values).
\end{enumerate}

\subsection{}
\begin{enumerate}[(a)]
    \item We have $f^\pi(x) = f(x^\pi) = \sumS \hat{f}(S) \chi_S(x^\pi) = \sumS \hat{f}(S) \chi_{\pi^{-1}(S)}(x)$, so
    \eqn{f^\pi = \sumS \hat{f}(S) \chi_{\pi^{-1}(S)} = \sumS \hat{f}\left(\pi^{-1}(S)\right) \chi_S}
    and thus $\widehat{f^\pi}(S) = \hat{f}(\pi^{-1}(S))$ for all $S$.
    
    \item Indeed: for any $\pi, \sigma \in P_n$, we have
    \eqn{f^\pi - f^\sigma = \sum_{i = 0}^k \sum_{\abs{S} = k} \left(\widehat{f^\pi}(S) - \widehat{f^\sigma}(S)\right)\chi_S = \sum_{i = 0}^k \sum_{\abs{S} = k} \left(\hat{f}\left(\pi^{-1}(S)\right) - \hat{f}\left(\sigma^{-1}(S)\right)\right)\chi_S,}
    and the fact that $\pi, \sigma \in P_k$ for all $k$ implies $\hat{f}\left(\pi^{-1}(S)\right) \geq \hat{f}\left(\sigma^{-1}(S)\right)$ as well as $\hat{f}\left(\sigma^{-1}(S)\right) \geq \hat{f}\left(\pi^{-1}(S)\right)$ for all $S$; therefore the right-hand side of the expression is 0, so that $f^\pi = f^\sigma$.
    
    \item Condition (i) holds by definition, since $\text{can}(f) = f^\pi$ for some permutation $\pi$. Condition (ii) holds because $g = f^\pi$ and $\sigma \in P_n$ with respect to $g$ if and only if $\sigma \circ \pi \in P_n$ with respect to $f$. Thus, $\text{can}(g) = g^\sigma = \left(f^\pi\right)^\sigma = f^{\sigma \circ \pi} = \text{can}(f)$.
    
    \item For each $i \in [n]$, computing $\hat{f}(i)$ takes $O(2^n)$ time, for a total complexity of $O(n 2^n) = \tilde{O}(2^n)$. Then, since under this condition there exists a single permutation $\pi \in S_n$ that makes the sequence $\left(\hat{f}(i)\right)_{i \in [n]}$ maximal (which can be obtained by sorting $n$ numbers, whose time is dominated by the computation of the $\hat{f}(i)$), the algorithm then outputs $f^\pi$.
    
    \item
\end{enumerate}

\subsection{}

\subsection{}

\subsection{}

\subsection{}

\subsection{}

\section{Chapter 2}

\subsection{}

\subsection{}

\subsection{}

\begin{enumerate}[(a)]
    \item ($\Longleftarrow$) Since $f(-x) = \sum_{i = 1}^n -x_i = - \sum_{i = 1}^n x_i = - f(x)$, the function is odd; and if $x \preceq y$, then $\sum x_i \leq \sum y_i$, so $f(x) \leq f(y)$ and thus $f$ is monotone.
    
    ($\Longrightarrow$) Since $f$ is symmetric, $f: [-n, n] \to \fbitset$, where $f(k) = f(x)$ for any $x$ such that $\sum x_i = k$ is well-defined; moreover, $f(k) \leq f(\ell)$ when $k \leq \ell$ by monotonicity. Taking $a \in [-n ,n]$ such that $f(a) = 1$ and $f(a - 1) = -1$ (if it exists; otherwise take $a_0 = -n$), then $f(x) = \textsf{sgn}\left(\frac1{2} - a + \sum_{i = 1}^n x_i\right)$.
    
    \item By the previous item, $f$ is a weighted majority with equal unit weights. By virtue of being odd, we thus have $f(-1) = -f(1)$ implying $f(k) = \textsf{sgn}(k)$, and $f = \text{Maj}_n$; moreover, $n$ is odd for otherwise we would have $f(0) = -f(0) \in \fbitset$, a contradiction.
\end{enumerate}

\subsection{}
($\Longrightarrow$) We have $f(x) = \textsf{sgn}\left(r - \frac1{2}\sum_i \abs{x_i - z_i}\right) = \textsf{sgn}\left(r - \frac1{2}\sum_i (1 - z_i \cdot x_i)\right) = \textsf{sgn}\left(r - \frac{n}{2} + \sum_i z_i \cdot x_i\right)$.

\noindent ($\Longleftarrow$) Set $r \coloneqq a_0 - \frac{n}{2}$ and $z \in \fbitset^n$ with $z_i = a_i$.

\subsection{}
\begin{enumerate}[(a)]
    \item We have $\Inf_i[f] = \E[x]{D_i f(x)^2} = \E[x]{\abs{D_i f(x)}} \geq \abs{\E[x]{D_i f(x)}} = \abs{\hf(i)}$. Moreover, equality holds iff the middle inequality is, in fact an equality; that is, if $\abs{D_i f(x)} = D_i f(x)$ for all $x$, which is equivalent to $f$ being unate in the \ith direction:
    \begin{align*}
        \forall x, \abs{D_i f(x)} = D_i f(x) &\iff \forall x, \abs{f(x^{i \mapsto 1}) - f(x^{i \mapsto 1})} = f(x^{i \mapsto 1}) - f(x^{i \mapsto 1})\\
        &\iff \forall x, f(x^{i \mapsto 1}) \geq f(x^{i \mapsto -1}) \text{ or } \forall x, f(x^{i \mapsto -1}) \geq f(x^{i \mapsto 1}).
    \end{align*}
    
    \item Indeed: if $f$ is unate, $\sumi \hf(i) \leq \sumi \abs{\hf(i)} = \sumi \Inf_i [f] = \I[f]$.
\end{enumerate}

\subsection{}

\subsection{}

\subsection{}

\subsection{}

\subsection{}

\subsection{}
We have $\Inf_i[f] = \sum_{i \in T} \hf(T)^2 \geq \hf(S)^2 > 0$.

\subsection{}
Using the fact that $\mathbf{\hf}(S)$ has mean 0 and variance $2^{-n}$ for every $S \subseteq [n]$, we have, for all $i \in [n]$:
\eqn{\E[\mathbf{f}]{\Inf_i[\mathbf{f}]} = \sum_{i \in S} \E[\mathbf{f}]{\mathbf{\hf}(S)^2} = 2^{n-1} \cdot 2^{-n} = \frac1{2},}
so $\E[\mathbf{f}]{\I[\mathbf{f}]} = \frac{n}{2}$.

\subsection{}
\TODO
\begin{enumerate}[(a)]
    \item We have
          \eqn{\E[x]{f(x)} = 2 \cdot \P[f(x) = 1] - 1 = 2 \cdot (1 - 2^{-w})^{2^w} - 1,}%{ \rightarrow \frac{2}{e} - 1 \approx }
          \eqn{\Var{}{f} = 1 - \E[x]{f(x)}^2 = 4 \cdot (1 - 2^{-w})^{2^{w + 1}} - 4 \cdot (1 - 2^{-w})^{2^w},}
          and, since $w \to \infty$ as $n \to \infty$, we obtain $\lim_{n \to \infty} \E[x]{f(x)} = \frac{2}{e} - 1 \approx -0.264$ as well as $\lim_{n \to \infty} \Var{}{f} = \frac{4}{e^2} - \frac{4}{e} \approx -0.93$.
          
    \item $D_1 f(x) = 0$ except when the first coordinate flips the value of $f$; that is, when all other members of its tribe vote $-1$ and all other tribes vote $1$.
    
    \item $\Inf_i[f] = \E[x]{D_i f(x)^2} = 2^{1-w} \cdot (1 - 2^{-w})^{2^w - 1} = \frac{2}{2^w - 1} \cdot (1 - 2^{-w})^{2^w}$. Since $w = \log n - \log w$, we have: $\lim_{n \to \infty} \Inf_i[f] = \Theta\left(2^{-w}\right) = \Theta\left(\frac{w}{n}\right) = \Theta\left(\frac{\log n}{n}\right)$; consequently, $\I[f] = \Theta(\log n)$.
\end{enumerate}

\subsection{}

\subsection{}

\subsection{}

\subsection{}

\subsection{}

\subsection{}

\subsection{}

\subsection{}

\subsection{}
\begin{enumerate}[(a)]
    \item Since $i$ is pivotal exactly when the $n - 1$ remaining voters divide their votes equally, we have $\Inf_i \left[\text{Maj}_n\right] = \binom{n-1}{\frac{n-1}{2}} \cdot 2^{1-n}$.
    
    \item Indeed:
    \begin{align*}
        \frac{\Inf_i \left[\text{Maj}_n\right]}{\Inf_i \left[\text{Maj}_{n+2}\right]} &= \frac{(n-1) \cdot (n - 2) \cdots \left(\frac{n-1}{2} + 1\right)}{\frac{n-1}{2} \cdots 2 \cdot 1} \cdot \frac{\frac{n+1}{2} \cdots 2 \cdot 1}{(n + 1) \cdot n \cdots \left(\frac{n+1}{2} + 1\right)} \cdot \frac{2^{1-n}}{2^{-1-n}}\\
        &= 4 \cdot \frac{(n-1)!}{(n+1)!} \cdot \left(\frac{n+1}{2}\right)^2\\
        &= \frac{n+1}{n} > 1.
    \end{align*}
    
    \item We have:
    \begin{align*}
        \Inf_i[\text{Maj}_n] &= \binom{n-1}{\frac{n-1}{2}} \cdot 2^{1-n}\\
        &= 2 \cdot \frac{(n-1)!}{\left(\left(\frac{n-1}{2}\right)!\right)^2} \cdot 2^{1-n}\\
        &= \cdot 2^{1-n} \frac{\left(\frac{n-1}{e}\right)^{n-1}}{\left(\frac{n-1}{2e}\right)^{n-1}} \cdot \frac{\sqrt{2 \pi (n-1)} + O(1/\sqrt{n})}{\pi (n-1) + O(1)}\\
        &= \sqrt{\frac{2}{\pi}} \cdot \frac1{\sqrt{n-1}} + O\left(\frac1{n^{3/2}}\right)\\
        &= \sqrt{\frac{2}{\pi}} \cdot \frac1{\sqrt{n}} + O\left(\frac1{n^{3/2}}\right).
    \end{align*}
    
    \item Since $f$ is monotone, $\W^1[f] = \sumi \hf(i)^2 = \sumi \Inf_i[f]^2$ by Proposition 2.21, and $\frac{2}{\pi n} \leq \Inf_i[f]^2 \leq \frac{2}{\pi n} + O(1/n^2)$, we have $2/\pi \leq n \cdot \Inf_1[f] = \W^1[f] \leq 2/\pi + O(1/n)$.
    
    \item $\sqrt{2/\pi} \cdot \sqrt{n} \leq \Inf_1[f] = \I[\text{Maj}_n] = \sqrt{2/\pi} \cdot \sqrt{n} + O(1/n)$.
    
    \item \TODO This follows from the fact that the \ith variable is influential when $n/2 - 1$ of the voters vote for $\text{Maj}_n(n/2)$ and the remaining $n/2$ voters vote for $-\text{Maj}_n(n/2)$. Thus,
    \eqn{\Inf_i[\text{Maj}_n] = \binom{n-1}{n/2} \cdot 2^{1-n} = \frac{\binom{n-1}{n/2}}{2 \cdot \binom{n-2}{n/2-1}} \cdot \Inf_i[\text{Maj}_{n-1}] = \left(1 - \frac1{n}\right) \cdot \Inf_i[\text{Maj}_{n-1}]}
\end{enumerate}

\subsection{}

\subsection{}

\subsection{}

\subsection{}

\subsection{}

\subsection{}

\subsection{}

\subsection{}

\subsection{}

\subsection{}

\subsection{}

\subsection{}
Indeed: $\abs{\T_\rho f(x)} = \abs{\E[y \sim N_\rho(x)]{f(y)}} \leq \E[x \sim N_\rho(y)]{\abs{f(y)}} = \T_\rho \abs{f}(x)$. Moreover, when $-1 < \rho < 1$, the support of $N_\rho(x)$ (for any $x$) is the entire hypercube $\fbitset^n$; therefore, equality implies $\abs{f(x)} \geq 0$ for all $x$, or, in other words, that $f$ is either non-negative or non-positive.

\section{Chapter 3}

\section{Chapter 4}

\section{Chapter 5}

\section{Chapter 6}

\section{Chapter 7}

\section{Chapter 8}

\section{Chapter 9}

\subsection{}

\subsection{}

Let $f(x) = \beta + \sumi \alpha_i x_i$. Then,

\begin{align*}
    \E[x]{f(x)^4} &= \beta^4 + \beta^3 \sumi \alpha_i \E[x]{x_i} + \beta^2 \sumi \sumj \alpha_i \alpha_j \E[x]{x_i x_j}\\
    &+ \beta \sumi \sumj \sumk \alpha_i \alpha_j \alpha_k \E[x]{x_i x_j x_k} + \sumi \sumj \sumk \suml \alpha_i \alpha_j \alpha_k \alpha_\ell \E[x]{x_i x_j x_k x_\ell}\\
    &= \beta^4 + \beta^2 \sumi \alpha_i^2 \E{x_i^2} + 2 \sum_{i \neq j} \alpha_i^2 \alpha_j^2 \E{x_i^2} \E{x_j^2} + \sumi \alpha_i^4 \E{x_i^4}\\
    &\leq \beta^4 + \beta^2 \sumi \alpha_i^2 \E{x_i^2} + 2 \sum_{i \neq j} \alpha_i^2 \alpha_j^2 \E{x_i^2} \E{x_j^2} + 3 \sumi \alpha_i^4 \E{x_i^2}^2\\
    &\leq \beta^4 + \beta^2 \sumi \alpha_i^2 \E{x_i^2} + 3 \sumi \sumj \alpha_i^2 \alpha_j^2 \E{x_i^2} \E{x_j^2}
\end{align*}
and

\begin{align*}
    \E[x]{f(x)^2}^2 &= \left(\beta^2 + \beta \sumi \alpha_i \E[x]{x_i} + \sumi \sumj \alpha_i \alpha_j \E[x]{x_i x_j}\right)^2\\
    &= \left(\beta^2 + \sumi \alpha_i^2 \E[x]{x_i^2}\right)^2\\
    &= \beta^4 + 2\beta^2 \sumi \alpha_i^2 \E[x]{x_i^2} + \sumi \sumj \alpha_i^4 \alpha_j^4 \E{x_i^2}^2 \E{x_j^2}^2.
\end{align*}

Comparing term by term, we see that $\E[x]{f(x)^4} \leq 3 \E[x]{f(x)^2}^2$; in fact, if the bits $x_i$ are 2-reasonable, then so is $f(x)$.

\subsection{}
\begin{enumerate}[(a)]
    \item We have

    \begin{align*}
        \E{f^4} &= \sum_{\substack{S,T,U,V \subseteq [n]\\\abs{S} = \abs{T} = \abs{U} = \abs{V} = k}} \E{x^S x^T x^U x^V}\\
        &\geq \sum_{\substack{A_1, A_2, A_3, A_4, A_5, A_6 \subseteq [n]\\\abs{A_i} = k/3,~ A_i \cap A_j = \varnothing\\S = A_1 \cup A_2 \cup A_3\\T = A_4 \cup A_5 \cup A_6\\U = A_1 \cup A_3 \cup A_4\\V = A_2 \cup A_5 \cup A_6}} \E{\left(x^S\right)^2 \left(x^T\right)^2}\\
        &= \binom{n}{k/3, k/3, k/3, k/3, k/3, k/3, n - 2k}.
    \end{align*}
    Since
    \eqn{\E{f^2} = \sum_{\substack{S, T \subseteq [n]\\\abs{S} = \abs{T} = k}} \E{x^S x^T} = \sum_{\substack{S [n]\\\abs{S} k}} \E{\left(x^S\right)^2} = \binom{n}{k},}
    the result follows.

    \item Indeed:
    
    \begin{align*}
        \binom{n}{k/3, k/3, k/3, k/3, k/3, k/3, n - 2k} &= \prod_{i = 0}^5 \binom{n - ik/3}{k/3}\\
        &= \prod_{i = 1}^5 \frac{(n - ik/3)!}{(k/3)! (n-(i+1)k/3)!}\\
        &= \frac{n!}{\left(\frac{k}{3}!\right)^6 (n - 2k)!}
    \end{align*}
    and
    
    \eqn{\binom{n}{k}^2 = \frac{n!^2}{k!^2 (n-k)!^2}}
    imply
    
    \begin{align*}
        \frac{\binom{n}{k/3, k/3, k/3, k/3, k/3, k/3, n - 2k}}{\binom{n}{k}^2} &= \frac{(n-k)!^2}{n! (n-2k)!} \cdot \frac{k!^2}{(k/3)!^6}\\
        &= \Theta\left(\frac{n-k}{\sqrt{n(n-2k)}} \cdot \left(1 - \frac{k}{n}\right)^n \cdot \left(1 + \frac{k}{n-2k}\right)^{n-2k} \cdot \frac{9^k}{k^2}\right).
    \end{align*}
    Using $\lim_{n \to \infty} (n-k)/\sqrt{n(n-2k)} = 1$, as well as $\lim_{n \to \infty}(1-k/n)^n = e^{-k}$ and $\lim_{n \to \infty}(1-k/(n-2k))^{n-2k} = e^k$, we conclude that the ratio is $\Theta(k^{-2} 9^k)$. Finally, since $\norm{f}_4^4 = \Omega(k^{-2}) \cdot 9^k \norm{f}_2^4$, taking fourth roots on both sides shows the desired lower bound.
\end{enumerate}
    
\subsection{}
As in the proof by induction of the Bonami lemma, we have:

\begin{align*}
    \E{f^4} &= \E{d^4} + 6 \E{d^2 e^2} + \E{e^4}\\
    &\leq \E{d^4} + 6 \sqrt{\E{d^4} \E{e^4}} + \E{e^4} \text{ and}\\
    \E{f^2} &= \E{d^2} + \E{e^2},
\end{align*}
and the induction hypothesis gives
    
\begin{align*}
    \E{f^4} &\leq \max\set{B,9}^{k-1}\left(\E{d^2}^2 + 6 \cdot \sqrt{\max\set{B,9}} \cdot \E{d^2} \cdot \E{e^2} + \max\set{B,9} \cdot \E{e^2}^2\right)\\
    &\leq \max\set{B,9}^k \cdot \left(\E{d^2} + \E{e^2}\right)^2 = \max\set{B,9}^k \cdot \E{f^2}^2.
\end{align*}

\subsection{}
Define $x = \sqrt{1 + 39 \sqrt{\delta}}$ and $y = \sqrt{1 - 39 \sqrt{\delta}}$, and note that $(x^2 - 1)^2/9 = (y^2 - 1)^2/9 = 169 \delta$. We perform a case analysis, where, for $\ell$, we have $\ell > x$, $\ell < -x$ or $-y < \ell < y$; and, for $f$, we have either $f = 1$ or $f = -1$.

The cases $\ell > x$ and $f = 1$, as well as $-y < \ell < y$ and $f = 1$, imply the remaining four. In the first, we have
\begin{align*}
    (f - \ell)^2 - 169 \delta &= (\ell - 1)^2 - \frac{(x^2 - 1)^2}{9}\\
    &> (x - 1)^2 - \frac{(x^2 - 1)^2}{9}\\
    &= (x - 1)^2 \left(1 - \frac{(x + 1)^2}{9}\right)\\
    &= - \frac{(x - 1)^2 (x - 2) (x + 4)}{9}\\
    &> 0,
\end{align*}
since $x \in (1,2)$. The second case is essentially the same:
\begin{align*}
    (f - \ell)^2 - 169 \delta &= (\ell - 1)^2 - \frac{(y^2 - 1)^2}{9}\\
    &> (y - 1)^2 - \frac{(y^2 - 1)^2}{9}\\
    &= - \frac{(y - 1)^2 (y - 2) (y + 4)}{9}\\
    &> 0,
\end{align*}
since $y \in (0,1)$.

\subsection{}
\begin{enumerate}[(a)]
    \item We have
    \begin{align*}
        \norm{\T_{\frac{1-\delta}{\sqrt{3}}} f}_4 &= \left(\sumk \norm{\T_{\frac{1-\delta}{\sqrt{3}}} f^{=k}}^4_4\right)^{1/4}\\
        &\leq \sumk \norm{\T_{\frac{1-\delta}{\sqrt{3}}} f^{=k}}_4\\
        &\leq \sumk \sqrt{3}^k \cdot \left(\frac{1-\delta}{\sqrt{3}}\right)^k \norm{f^{=k}}_2\\
        &= \sumk (1 - \delta)^k \norm{f^{=k}}_2,
    \end{align*}
    where the last inequality is the Bonami lemma. \TODO
    
    \item Indeed:
    \begin{align*}
        \norm{\T_\rho(g^{\oplus d})}_p &= \E[x]{\E[y \sim N_\rho(x)]{g^{\oplus d}(y)}^p}^{1/p}\\
        &= \E[x^{(1)}, \ldots, x^{(d)}]{\E[y^{(1)} \sim N_\rho(x^{(1)}), \ldots, y^{(d)} \sim N_\rho(x^{(d)})]{\prod_{i \in [d]} g(y^{(i)})}^p}^{1/p}\\
        &= \E[x^{(1)}, \ldots, x^{(d)}]{ \prod_{i \in [d]} \E[y^{(i)} \sim N_\rho(x^{(i)})]{g(y^{(i)})}^p}^{1/p}\\
        &= \prod_{i \in [d]}\E[x^{(i)}]{\E[y^{(i)} \sim N_\rho(x^{(i)})]{g(y^{(i)})}^p}^{1/p}\\
        &= \prod_{i \in [d]} \norm{\T_\rho g}_p\\
        &= \norm{\T_\rho g}_p^d.
    \end{align*}
    
    \item For any $d \in \N$, we have
    \begin{align*}
        \norm{\T_{\frac{1-\delta}{\sqrt{3}}} f^{\oplus d}}_4 &= \norm{\T_{\frac{1-\delta}{\sqrt{3}}} f}^d\\
        &\leq \frac1{\delta} \norm{f^{\oplus d}}_2\\
        &= \frac1{\delta} \norm{f}_2^d,
    \end{align*}
    so that $\norm{\T_{\frac{1-\delta}{\sqrt{3}}} f} \leq \delta^{-1/d} \norm{f}_2$. Taking $d \to \infty$ proves the inequality.
    
    \item Continuity of the norm and of the noise operator (as a function of $\delta$, with $f$ fixed) implies $\norm{\T_{\frac1{\sqrt{3}}} f}_4 = \lim_{\delta \to 0^+} \norm{\T_{\frac{1-\delta}{\sqrt{3}}} f}_4 \leq \norm{f}_2$.
\end{enumerate}

\subsection{}
Since $\abs{\T_\rho f} \leq T_\rho \abs{f}$ pointwise, we have $\norm{T_\rho f}_q \leq \norm{T_\rho \abs{f}}_q$. Proving the inequality for non-negative functions would show $\norm{T_\rho \abs{f}}_q \leq \norm{\abs{f}}_p = \norm{f}_p$, and the result follows.

\subsection{}

\subsection{}
\begin{enumerate}[(a)]
    \item Indeed: taking the coefficient multiplying $X$ to be $bc$, we have $\norm{a + b \rho (c X)}_q \leq \norm{a + b (c X)}_p$ for all $a, b \in \R$, i.e., $cX$ is $(p, q, \rho)$-hypercontractive.
    
    \item Taking $a = 0$ and $b = 1$ in the statement of hypercontractivity, we have $\rho \norm{X}_q = \norm{\rho X}_q \leq \norm{X}_p$. Dividing by $\norm{X}_q$ yields the inequality (assuming the random variable is not identically zero).
\end{enumerate}

\subsection{}
\begin{enumerate}[(a)]
    \item Discreteness implies $X$ is bounded, and the latter is enough for the expectation of error terms to have the same order. Since $1 + \rho \eps X > 0$ for small enough $\abs{\eps}$, we have, for any $1 \leq r \leq \infty$,
    \eqn{\norm{1 + \rho \eps X}_r = \left(1 + r \rho \eps \E{X} + \frac{r (r - 1) \rho^2}{2} \eps^2 \E{X^2} + O(\eps^3)\right)^{1/r}.}
    
    Therefore, by the $(p,q,\rho)$-hypercontractivity of $X$ (which implies $(p,q,\abs{\eps} \rho)$-hypercontractivity as well),
    \begin{align*}
        1 + q \rho \eps \E{X} + \frac{q (q - 1) \rho^2}{2} \eps^2 \E{X^2} &\leq \left(1 + p \eps \E{X} + \frac{p (p - 1)}{2} \eps^2 \E{X^2} + O(\eps^3)\right)^{q/p}\\
        &=  1 + q \eps \E{X} + \frac{q (p - 1)}{2} \eps^2 \E{X^2} +  O(\eps^3),
    \end{align*}
    
    by taking another Taylor expansion. Rearranging, we have $\eps \E{X} \leq \alpha \eps^2 + O(\eps^3)$ for some nonzero $\alpha \in \R$. Thus taking the limits $\eps \to 0^+$ and $\eps \to 0^-$ imply $\E{X} \geq 0$ and $\E{X} \leq 0$ (not necessarily in this order, so equality follows.
    
    \item Taking the Taylor expansion of the previous item and plugging in $\E{X} = 0$ yields
    \eqn{\frac{q (q - 1) \rho^2}{2} \eps^2 \E{X^2} \leq \frac{q (p - 1)}{2} \eps^2 \E{X^2} +  O(\eps^3).}
    Dividing by $\eps^2$ and rearranging,
    \eqn{\left((q - 1)\rho^2 - (p - 1)\right) \frac{q \E{X}}{2} \leq O(\eps).}
    For this to hold for all $\eps$, the right-hand side must be non-positive; equivalently,
    \eqn{(q - 1) \rho^2 \leq p - 1 \iff \rho \leq \sqrt{\frac{p - 1}{q - 1}}.} 
\end{enumerate}

\subsection{}
\begin{enumerate}[(a)]
    \item We need to show that, for every $a \in \R$ and $q \geq 1$, the inequality $\abs{a} = \norm{a}_q \leq \norm{a + X}_q$. By monotonicity of norms, $\norm{a + X}_q \geq \norm{a + X}_1$, so we need only show the inequality for $q = 1$. But $\norm{a + X}_1 = \E{\abs{a + X}} \geq \abs{a + \E{X}} = \abs{a}$, since $\E{X} = 0$.
    
    \item We have
    \begin{align*}
        \norm{a + \rho X}_q &\leq (1 - \rho) \abs{a} + \rho \norm{a + X}_q\\
        &\leq (1 - \rho) \norm{a + X}_q + \rho \norm{a + X}_q\\
        &= \norm{a + X}_q,
    \end{align*}
    where the second inequality follows from the previous item.
    
    \item We know, from the previous exercise, that $(p, q, \rho)$ hypercontractivity of $X$ implies $\E{X} = 0$; so, by item (b), $X$ is also $(q,q,\rho'/\rho)$-hypercontractive for every $\rho' < \rho$ (since $\rho'/\rho < 1$). Thus,
    \begin{align*}
        \norm{a + \rho' X}_q &= \rho \norm{\frac{a}{\rho} + \frac{\rho'}{\rho} X}_q\\
        &\leq \rho \norm{\frac{a}{\rho} + X}_q\\
        &= \norm{a + \rho X}_q\\
        &\leq \norm{a + X}_p.
    \end{align*}
\end{enumerate}

\end{document}
